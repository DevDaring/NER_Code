{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fi96VKH8dASM",
    "outputId": "3783e0d3-ed03-415d-f74d-22eed9be9abb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-crf in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.7.2)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: seqeval in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (4.66.5)\n",
      "Requirement already satisfied: wandb in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.19.3)\n",
      "Requirement already satisfied: GPUtil in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: matplotlib in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.24.5)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (72.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-crf datasets seqeval tqdm wandb GPUtil matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SdylUOAQZZ1z",
    "outputId": "dfc059bf-cf53-4ede-866e-9e5c3e191565"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizerFast, XLMRobertaModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from huggingface_hub import HfFolder, HfApi\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgYTewwnehgK",
    "outputId": "f38eb182-20e4-4b7b-e577-b2e7a5d17199"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_to_id = {\n",
    "    'B-geo': 0, 'O': 1, 'B-gpe': 2, 'B-per': 3, 'I-per': 4, 'B-tim': 5,\n",
    "    'B-org': 6, 'I-org': 7, 'B-art': 8, 'I-art': 9, 'I-tim': 10,\n",
    "    'B-eve': 11, 'I-eve': 12, 'I-geo': 13, 'I-gpe': 14, 'B-nat': 15, 'I-nat': 16\n",
    "}\n",
    "\n",
    "from huggingface_hub import HfFolder\n",
    "# Set your HuggingFace token\n",
    "HF_TOKEN = \"secret\"  # Replace with your token\n",
    "HfFolder.save_token(HF_TOKEN)\n",
    "wandb_key='secret'  # Your W&B API key\n",
    "wandb.login(key=wandb_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "S9_tBoQKdNsC"
   },
   "outputs": [],
   "source": [
    "class SentenceLengthAnalyzer:\n",
    "    def __init__(self, file_path, tokenizer):\n",
    "        self.file_path = file_path\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def calculate_max_length(self):\n",
    "        \"\"\"Calculate the 99th percentile of token lengths in the dataset\"\"\"\n",
    "        logger.info(\"Analyzing sequence lengths...\")\n",
    "        df = pd.read_csv(self.file_path)\n",
    "        sentences = df.groupby('Sentence #')['Word'].apply(list).values\n",
    "\n",
    "        lengths = []\n",
    "        for sentence in tqdm(sentences, desc=\"Calculating sequence lengths\"):\n",
    "            try:\n",
    "                tokens = self.tokenizer(sentence, is_split_into_words=True, truncation=False)\n",
    "                lengths.append(len(tokens['input_ids']))\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing sentence: {e}\")\n",
    "                continue\n",
    "\n",
    "        max_len = int(np.percentile(lengths, 99))\n",
    "        logger.info(f\"Sequence length statistics:\")\n",
    "        logger.info(f\"Mean length: {np.mean(lengths):.2f}\")\n",
    "        logger.info(f\"Median length: {np.median(lengths):.2f}\")\n",
    "        logger.info(f\"99th percentile length: {max_len}\")\n",
    "        logger.info(f\"Max length: {max(lengths)}\")\n",
    "\n",
    "        return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HJRn3agxdZV-"
   },
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, tags, tokenizer, tag_to_id, max_len):\n",
    "        self.texts = texts\n",
    "        self.tags = tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag_to_id = tag_to_id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            words = self.texts[idx]\n",
    "            tags = self.tags[idx]\n",
    "\n",
    "            encoding = self.tokenizer(\n",
    "                words,\n",
    "                is_split_into_words=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_len,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            label_ids = []\n",
    "            word_ids = encoding.word_ids()\n",
    "\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                else:\n",
    "                    label_ids.append(self.tag_to_id[tags[word_idx]])\n",
    "\n",
    "            encoding = {key: val.squeeze() for key, val in encoding.items()}\n",
    "            encoding['labels'] = torch.tensor(label_ids)\n",
    "\n",
    "            return encoding\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing item {idx}: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hVPkXMi9daG_"
   },
   "outputs": [],
   "source": [
    "class XLMRobertaBiLSTM(nn.Module):\n",
    "    def __init__(self, num_labels, dropout=0.1, lstm_hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.roberta = XLMRobertaModel.from_pretrained('xlm-roberta-large')\n",
    "        # Get the actual hidden size from the model config\n",
    "        hidden_size = self.roberta.config.hidden_size  # This will be 1024 for xlm-roberta-large\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,  # Using the actual hidden size (1024)\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if 2 > 1 else 0\n",
    "        )\n",
    "        self.classifier = nn.Linear(lstm_hidden_size * 2, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get RoBERTa outputs\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]  # Shape: [batch_size, seq_len, hidden_size(1024)]\n",
    "\n",
    "        # Apply dropout\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "        # Pass through BiLSTM\n",
    "        lstm_output, _ = self.lstm(sequence_output)  # Shape: [batch_size, seq_len, lstm_hidden_size*2]\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "\n",
    "        # Get logits\n",
    "        logits = self.classifier(lstm_output)  # Shape: [batch_size, seq_len, num_labels]\n",
    "\n",
    "        # Calculate loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # Only consider loss on valid tokens\n",
    "            active_loss = labels.view(-1) != -100\n",
    "            active_logits = logits.view(-1, logits.shape[-1])\n",
    "            active_labels = labels.view(-1)\n",
    "            loss = loss_fct(active_logits[active_loss], active_labels[active_loss])\n",
    "\n",
    "        return {'loss': loss, 'logits': logits} if loss is not None else {'logits': logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Fx-2Iawkdxjj"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "class NERTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=10):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Validate the model on the validation set\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_pbar = tqdm(self.val_loader, desc='Validation')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_pbar:\n",
    "                try:\n",
    "                    input_ids = batch['input_ids'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    labels = batch['labels'].to(self.device)\n",
    "\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels\n",
    "                    )\n",
    "\n",
    "                    loss = outputs['loss']\n",
    "                    total_val_loss += loss.item()\n",
    "\n",
    "                    val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "                except RuntimeError as e:\n",
    "                    if \"out of memory\" in str(e):\n",
    "                        logger.warning(\"CUDA OOM during validation. Clearing cache...\")\n",
    "                        if hasattr(torch.cuda, 'empty_cache'):\n",
    "                            torch.cuda.empty_cache()\n",
    "                        continue\n",
    "                    raise e\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(self.val_loader)\n",
    "        return avg_val_loss\n",
    "\n",
    "    def train(self):\n",
    "        best_val_loss = float('inf')\n",
    "        accumulated_batches = 2  # Reduced for small dataset\n",
    "\n",
    "        try:\n",
    "            self.model.roberta.gradient_checkpointing_enable()\n",
    "\n",
    "            for epoch in range(self.num_epochs):\n",
    "                # Training\n",
    "                self.model.train()\n",
    "                total_train_loss = 0\n",
    "                train_pbar = tqdm(self.train_loader,\n",
    "                                desc=f'Epoch {epoch + 1}/{self.num_epochs} [Train]')\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                for batch_idx, batch in enumerate(train_pbar):\n",
    "                    try:\n",
    "                        input_ids = batch['input_ids'].to(self.device)\n",
    "                        attention_mask = batch['attention_mask'].to(self.device)\n",
    "                        labels = batch['labels'].to(self.device)\n",
    "\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels\n",
    "                        )\n",
    "\n",
    "                        loss = outputs['loss'] / accumulated_batches\n",
    "                        total_train_loss += loss.item() * accumulated_batches\n",
    "\n",
    "                        loss.backward()\n",
    "\n",
    "                        if (batch_idx + 1) % accumulated_batches == 0:\n",
    "                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                            self.optimizer.step()\n",
    "                            self.scheduler.step()\n",
    "                            self.optimizer.zero_grad()\n",
    "\n",
    "                        train_pbar.set_postfix({\n",
    "                            'loss': f'{loss.item() * accumulated_batches:.4f}'\n",
    "                        })\n",
    "\n",
    "                        # Log to wandb with reduced frequency\n",
    "                        if batch_idx % 50 == 0:  # Reduced frequency for small dataset\n",
    "                            wandb.log({\n",
    "                                \"train_batch_loss\": loss.item() * accumulated_batches,\n",
    "                                \"learning_rate\": self.scheduler.get_last_lr()[0],\n",
    "                            })\n",
    "\n",
    "                    except RuntimeError as e:\n",
    "                        if \"out of memory\" in str(e):\n",
    "                            logger.warning(f\"CUDA OOM in batch {batch_idx}. Skipping...\")\n",
    "                            torch.cuda.empty_cache()\n",
    "                            gc.collect()\n",
    "                            self.optimizer.zero_grad()\n",
    "                            continue\n",
    "                        raise e\n",
    "\n",
    "                avg_train_loss = total_train_loss / len(self.train_loader)\n",
    "\n",
    "                # Validation\n",
    "                val_loss = self.validate()\n",
    "\n",
    "                # Log epoch metrics\n",
    "                wandb.log({\n",
    "                    \"train_epoch_loss\": avg_train_loss,\n",
    "                    \"val_epoch_loss\": val_loss,\n",
    "                    \"epoch\": epoch\n",
    "                })\n",
    "\n",
    "                # Early stopping check\n",
    "                self.early_stopping(val_loss)\n",
    "\n",
    "                # Save best model\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    self.save_checkpoint(\n",
    "                        f\"best_model_epoch_{epoch}_valloss_{val_loss:.4f}.pt\"\n",
    "                    )\n",
    "\n",
    "                logger.info(f'Epoch {epoch + 1} Summary:')\n",
    "                logger.info(f'Average training loss: {avg_train_loss:.4f}')\n",
    "                logger.info(f'Average validation loss: {val_loss:.4f}')\n",
    "\n",
    "                if self.early_stopping.early_stop:\n",
    "                    logger.info(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during training: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        \"\"\"Save a checkpoint of the model\"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'epoch': self.num_epochs,\n",
    "        }, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hmG4qDjAd2B7"
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "\n",
    "class ModelPublisher:\n",
    "    def __init__(self, model_id, token):\n",
    "        self.model_id = model_id\n",
    "        self.token = token\n",
    "        self.api = HfApi()\n",
    "\n",
    "    def push_to_hub(self, model, tokenizer):\n",
    "        try:\n",
    "            # Create temporary directory\n",
    "            os.makedirs(\"temp_model\", exist_ok=True)\n",
    "\n",
    "            # Save model state dict\n",
    "            torch.save(model.state_dict(), \"temp_model/pytorch_model.bin\")\n",
    "\n",
    "            # Save config\n",
    "            config = {\n",
    "                \"architectures\": [\"XLMRobertaBiLSTM\"],\n",
    "                \"model_type\": \"xlm-roberta-bilstm\",\n",
    "                \"num_labels\": model.classifier.out_features,\n",
    "                \"lstm_hidden_size\": model.lstm.hidden_size,\n",
    "                \"dropout\": model.dropout.p,\n",
    "                \"base_model\": \"xlm-roberta-large\",\n",
    "            }\n",
    "\n",
    "            with open(\"temp_model/config.json\", \"w\") as f:\n",
    "                json.dump(config, f)\n",
    "\n",
    "            # Delete the repository if it exists\n",
    "            try:\n",
    "                self.api.delete_repo(self.model_id, token=self.token)\n",
    "                logger.info(f\"Deleted existing repository: {self.model_id}\")\n",
    "            except Exception as e:\n",
    "                logger.info(f\"Repository doesn't exist or couldn't be deleted: {str(e)}\")\n",
    "\n",
    "            # Create new repository\n",
    "            self.api.create_repo(self.model_id, token=self.token)\n",
    "\n",
    "            # Upload files\n",
    "            self.api.upload_file(\n",
    "                path_or_fileobj=\"temp_model/pytorch_model.bin\",\n",
    "                path_in_repo=\"pytorch_model.bin\",\n",
    "                repo_id=self.model_id,\n",
    "                token=self.token\n",
    "            )\n",
    "\n",
    "            self.api.upload_file(\n",
    "                path_or_fileobj=\"temp_model/config.json\",\n",
    "                path_in_repo=\"config.json\",\n",
    "                repo_id=self.model_id,\n",
    "                token=self.token\n",
    "            )\n",
    "\n",
    "            # Push tokenizer files\n",
    "            tokenizer.save_pretrained(\"temp_model\")\n",
    "            tokenizer_files = [f for f in os.listdir(\"temp_model\") if f.startswith(\"tokenizer\") or f.endswith(\".json\")]\n",
    "\n",
    "            for file in tokenizer_files:\n",
    "                self.api.upload_file(\n",
    "                    path_or_fileobj=f\"temp_model/{file}\",\n",
    "                    path_in_repo=file,\n",
    "                    repo_id=self.model_id,\n",
    "                    token=self.token\n",
    "                )\n",
    "\n",
    "            # Clean up\n",
    "            import shutil\n",
    "            shutil.rmtree(\"temp_model\")\n",
    "\n",
    "            logger.info(f\"Successfully pushed model to {self.model_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error pushing to hub: {str(e)}\")\n",
    "            # Clean up on error\n",
    "            if os.path.exists(\"temp_model\"):\n",
    "                shutil.rmtree(\"temp_model\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686,
     "referenced_widgets": [
      "bc61ec67db7348eb8250e53ac0e88092",
      "c541927c8ec54775b09c0d2fff45b130",
      "6739a7301500410fab94144d5fd2c652",
      "d1ced12f3ed54a0186ed46cc94e202f1",
      "3006f612db344bc78d950116d223d954",
      "99158fdbe3e2433ab1c2ac6e0b34ba8d",
      "0f3b118c51904cf682452fb8bbe4608c",
      "ea49321593cf47c08108b25f926287b1",
      "f2527d8c162f42c7bd1bebdd0c0bdc07",
      "8e2a123ebf354b72849d32d9e619978a",
      "196c1344837448b886a964e3effec00d",
      "377a5406e4e546d9a5468696dfaac0e0",
      "df4c4328930d41d1a2671dc866d6991f",
      "e50cdc4d95aa4bcf9657c0c025eb01c8",
      "9be769e2e057495bb06b1d0131baae0f",
      "c692fe9eeefd4883be941ca4b8eece8d",
      "43c90cce99664de28746950a51d646ef",
      "dbe5db16cc204b5d8d800822a88f8199",
      "600b850293be41faa99595e80af0dbd2",
      "bd8e909fbf7b446788eeb59c6bbe911c",
      "53ab1b628ad94edd9a99b24ea74195f9",
      "5420b269ad2944f7a2fb00ca9721e0c0"
     ]
    },
    "id": "Xc5SidTed6GD",
    "outputId": "acad9e91-e572-4b1e-93bc-61d4cccd4e29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cuda\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wandb/run-20250114_090855-4hnqfqy7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deb/bengali_ner/runs/4hnqfqy7' target=\"_blank\">Ben_NER_xlm-roberta-large_BiLSTM_20250114_090855_NVIDIA RTX A5000</a></strong> to <a href='https://wandb.ai/deb/bengali_ner' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deb/bengali_ner' target=\"_blank\">https://wandb.ai/deb/bengali_ner</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deb/bengali_ner/runs/4hnqfqy7' target=\"_blank\">https://wandb.ai/deb/bengali_ner/runs/4hnqfqy7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "INFO:__main__:Analyzing sequence lengths...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a27de749844ec499f87852c6a365f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating sequence lengths:   0%|          | 0/17715 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "INFO:__main__:Sequence length statistics:\n",
      "INFO:__main__:Mean length: 26.82\n",
      "INFO:__main__:Median length: 23.00\n",
      "INFO:__main__:99th percentile length: 91\n",
      "INFO:__main__:Max length: 599\n",
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff7b325738b46228157e941cb43ea68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 [Train]:   0%|          | 0/886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9453d5b381648aba624f6f412d4282b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 1 Summary:\n",
      "INFO:__main__:Average training loss: 0.7490\n",
      "INFO:__main__:Average validation loss: 0.3821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4e05d0a7cb4fb0b2d0ab67436c77ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 [Train]:   0%|          | 0/886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e47692f7b234fd0a7f35be5af42527b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 2 Summary:\n",
      "INFO:__main__:Average training loss: 0.2883\n",
      "INFO:__main__:Average validation loss: 0.2298\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f430e2a353014674ac45f0e6c5463510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 [Train]:   0%|          | 0/886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf8a80015a945b4ad4887eb35c607a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 3 Summary:\n",
      "INFO:__main__:Average training loss: 0.1778\n",
      "INFO:__main__:Average validation loss: 0.1701\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3c79944e0e4d7c98006835550a3526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 [Train]:   0%|          | 0/886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2aa4180aff4879b735d3c7fdf8becb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 4 Summary:\n",
      "INFO:__main__:Average training loss: 0.1249\n",
      "INFO:__main__:Average validation loss: 0.1543\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eec4515caca4d4b858bef02b86b4daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 [Train]:   0%|          | 0/886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a96f87054a4e519bcf9715fc5f5186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 5 Summary:\n",
      "INFO:__main__:Average training loss: 0.0988\n",
      "INFO:__main__:Average validation loss: 0.1381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbaae17d8004e74ab2ab30e290dd41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 [Train]:   0%|          | 0/886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc14670204904b5ea09df114f51e5709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 6 Summary:\n",
      "INFO:__main__:Average training loss: 0.0795\n",
      "INFO:__main__:Average validation loss: 0.1417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9159bfbcc94ac2b345aefb41f605b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 [Train]:   0%|          | 0/886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f4bef6eaa047fc8386a19dab8e8bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 7 Summary:\n",
      "INFO:__main__:Average training loss: 0.0633\n",
      "INFO:__main__:Average validation loss: 0.1497\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0203c03e43d4ee7900bf79def1426d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 [Train]:   0%|          | 0/886 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fd2b4db6d048e19e871208d3224864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 8 Summary:\n",
      "INFO:__main__:Average training loss: 0.0552\n",
      "INFO:__main__:Average validation loss: 0.1505\n",
      "INFO:__main__:Early stopping triggered\n",
      "INFO:__main__:Deleted existing repository: Debk/Ben_NER_xlm-roberta-large_BiLSTM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8d8010f72e41f59473bc358bf4ecd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808fa1ecd426404fb7fac836f3076cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully pushed model to Debk/Ben_NER_xlm-roberta-large_BiLSTM\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>learning_rate</td><td>███▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_batch_loss</td><td>█▆█▆▅▃▂▃▂▃▃▃▂▃▁▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁</td></tr><tr><td>train_epoch_loss</td><td>█▃▂▂▁▁▁▁</td></tr><tr><td>val_epoch_loss</td><td>█▄▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>train_batch_loss</td><td>0.00773</td></tr><tr><td>train_epoch_loss</td><td>0.0552</td></tr><tr><td>val_epoch_loss</td><td>0.15048</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Ben_NER_xlm-roberta-large_BiLSTM_20250114_090855_NVIDIA RTX A5000</strong> at: <a href='https://wandb.ai/deb/bengali_ner/runs/4hnqfqy7' target=\"_blank\">https://wandb.ai/deb/bengali_ner/runs/4hnqfqy7</a><br> View project at: <a href='https://wandb.ai/deb/bengali_ner' target=\"_blank\">https://wandb.ai/deb/bengali_ner</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250114_090855-4hnqfqy7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#def main():\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Set environmental variables for CUDA debugging\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Initialize wandb\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "gpu_type = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "wandb_run_name = f\"Ben_NER_xlm-roberta-large_BiLSTM_{current_time}_{gpu_type}\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"bengali_ner\",\n",
    "    name=wandb_run_name,\n",
    "    config={\n",
    "        \"model_name\": \"xlm-roberta-large\",\n",
    "        \"architecture\": \"BiLSTM\",\n",
    "        \"max_length\": \"99th percentile\",\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 2e-5\n",
    "    }\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-large')\n",
    "\n",
    "    # Calculate max length\n",
    "    analyzer = SentenceLengthAnalyzer('b-ner-train.csv', tokenizer)\n",
    "    max_len = analyzer.calculate_max_length()\n",
    "\n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv('b-ner-train.csv')\n",
    "    sentences = df.groupby('Sentence #')['Word'].apply(list).values\n",
    "    tags = df.groupby('Sentence #')['Tag'].apply(list).values\n",
    "\n",
    "    # Split data\n",
    "    train_texts, val_texts, train_tags, val_tags = train_test_split(\n",
    "        sentences, tags, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = NERDataset(train_texts, train_tags, tokenizer, tag_to_id, max_len)\n",
    "    val_dataset = NERDataset(val_texts, val_tags, tokenizer, tag_to_id, max_len)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "    # Initialize model\n",
    "    model = XLMRobertaBiLSTM(num_labels=len(tag_to_id))\n",
    "    model.to(device)\n",
    "\n",
    "    # Setup training\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    num_training_steps = 10 * len(train_loader)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    # Setup training\n",
    "    trainer = NERTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        num_epochs=10  # Increased epochs\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Push to Hub\n",
    "    publisher = ModelPublisher(\n",
    "        model_id=\"Debk/Ben_NER_xlm-roberta-large_BiLSTM\",\n",
    "        token=\"secret\"\n",
    "    )\n",
    "    publisher.push_to_hub(model, tokenizer)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in main execution: {str(e)}\")\n",
    "    raise e\n",
    "finally:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSqhdVdChfNh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Data Analysis ===\n",
      "\n",
      "Tag Distribution in Training Data:\n",
      "O: 198077 (83.50%)\n",
      "B-geo: 8256 (3.48%)\n",
      "I-per: 6904 (2.91%)\n",
      "B-per: 6881 (2.90%)\n",
      "B-org: 4198 (1.77%)\n",
      "B-tim: 3787 (1.60%)\n",
      "I-org: 3404 (1.44%)\n",
      "B-gpe: 2003 (0.84%)\n",
      "I-tim: 1957 (0.83%)\n",
      "I-geo: 507 (0.21%)\n",
      "B-art: 336 (0.14%)\n",
      "B-eve: 327 (0.14%)\n",
      "I-eve: 274 (0.12%)\n",
      "I-art: 246 (0.10%)\n",
      "B-nat: 29 (0.01%)\n",
      "I-nat: 15 (0.01%)\n",
      "I-gpe: 7 (0.00%)\n",
      "\n",
      "Suggested Class Weights for Loss Function:\n",
      "O: 0.0704\n",
      "B-geo: 1.6901\n",
      "I-per: 2.0211\n",
      "B-per: 2.0278\n",
      "B-org: 3.3238\n",
      "B-tim: 3.6846\n",
      "I-org: 4.0991\n",
      "B-gpe: 6.9663\n",
      "I-tim: 7.1300\n",
      "I-geo: 27.5215\n",
      "B-art: 41.5280\n",
      "B-eve: 42.6710\n",
      "I-eve: 50.9249\n",
      "I-art: 56.7212\n",
      "B-nat: 481.1521\n",
      "I-nat: 930.2275\n",
      "I-gpe: 1993.3445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Output Verification ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "class TrainingVerifier:\n",
    "    def __init__(self, train_file_path, tag_to_id):\n",
    "        self.train_file_path = train_file_path\n",
    "        self.tag_to_id = tag_to_id\n",
    "        self.id_to_tag = {v: k for k, v in tag_to_id.items()}\n",
    "        \n",
    "    def analyze_training_data(self):\n",
    "        \"\"\"Analyze training data distribution\"\"\"\n",
    "        print(\"\\n=== Training Data Analysis ===\")\n",
    "        \n",
    "        # Load training data\n",
    "        df = pd.read_csv(self.train_file_path)\n",
    "        \n",
    "        # Analyze tag distribution\n",
    "        tag_counts = df['Tag'].value_counts()\n",
    "        total_tags = len(df)\n",
    "        \n",
    "        print(\"\\nTag Distribution in Training Data:\")\n",
    "        for tag, count in tag_counts.items():\n",
    "            percentage = (count / total_tags) * 100\n",
    "            print(f\"{tag}: {count} ({percentage:.2f}%)\")\n",
    "            \n",
    "        # Calculate class weights for loss function\n",
    "        class_weights = {\n",
    "            self.tag_to_id[tag]: (1.0 / count) * (total_tags / len(self.tag_to_id))\n",
    "            for tag, count in tag_counts.items()\n",
    "        }\n",
    "        \n",
    "        print(\"\\nSuggested Class Weights for Loss Function:\")\n",
    "        for tag_id, weight in class_weights.items():\n",
    "            print(f\"{self.id_to_tag[tag_id]}: {weight:.4f}\")\n",
    "            \n",
    "        return class_weights\n",
    "        \n",
    "    def verify_model_outputs(self, model, tokenizer, sample_sentence):\n",
    "        \"\"\"Verify model output distribution before training\"\"\"\n",
    "        print(\"\\n=== Model Output Verification ===\")\n",
    "        \n",
    "        # Tokenize sample sentence\n",
    "        encoding = tokenizer(\n",
    "            sample_sentence,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "            logits = outputs['logits']\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "            \n",
    "        # Analyze output distribution\n",
    "        print(\"\\nOutput Distribution for Sample Sentence:\")\n",
    "        for word, word_probs in zip(sample_sentence, probs):\n",
    "            top_probs, top_ids = torch.topk(word_probs, 3)\n",
    "            print(f\"\\nWord: {word}\")\n",
    "            for prob, tag_id in zip(top_probs, top_ids):\n",
    "                print(f\"{self.id_to_tag[tag_id.item()]}: {prob.item():.4f}\")\n",
    "                \n",
    "        # Check if outputs are too concentrated\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)\n",
    "        print(f\"\\nAverage Output Entropy: {entropy.mean().item():.4f}\")\n",
    "        if entropy.mean() < 0.5:\n",
    "            print(\"WARNING: Low entropy indicates model might be too confident!\")\n",
    "            \n",
    "    def suggest_training_improvements(self, class_weights):\n",
    "        \"\"\"Suggest improvements based on analysis\"\"\"\n",
    "        print(\"\\n=== Training Improvement Suggestions ===\")\n",
    "        \n",
    "        # Check for extreme class imbalance\n",
    "        weight_values = np.array(list(class_weights.values()))\n",
    "        weight_ratio = np.max(weight_values) / np.min(weight_values)\n",
    "        \n",
    "        if weight_ratio > 10:\n",
    "            print(\"\\n1. Severe class imbalance detected. Suggested fixes:\")\n",
    "            print(\"   - Use weighted loss function with calculated class weights\")\n",
    "            print(\"   - Consider data augmentation for minority classes\")\n",
    "            print(\"   - Implement stratified sampling in DataLoader\")\n",
    "            \n",
    "        print(\"\\n2. General Training Recommendations:\")\n",
    "        print(\"   - Start with a smaller learning rate (1e-5)\")\n",
    "        print(\"   - Use learning rate warmup\")\n",
    "        print(\"   - Implement gradient clipping\")\n",
    "        print(\"   - Monitor validation loss for early stopping\")\n",
    "        \n",
    "        # Suggested loss function code\n",
    "        print(\"\\n3. Suggested Loss Function Implementation:\")\n",
    "        print(\"\"\"\n",
    "        # Convert class weights to tensor\n",
    "        class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "        \n",
    "        # Define weighted loss function\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "        \n",
    "        # In forward pass\n",
    "        loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "        \"\"\")\n",
    "        \n",
    "def verify_training():\n",
    "    # Initialize verifier\n",
    "    verifier = TrainingVerifier('b-ner-train.csv', tag_to_id)\n",
    "    \n",
    "    # Analyze training data\n",
    "    class_weights = verifier.analyze_training_data()\n",
    "    \n",
    "    # Create sample sentence for testing\n",
    "    sample_sentence = [\"বাংলাদেশে \", \"পরিস্থিতি\", \"সঙ্কটাপন্ন\"]\n",
    "    \n",
    "    # Initialize model\n",
    "    model = XLMRobertaBiLSTM(num_labels=len(tag_to_id))\n",
    "    tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-large')\n",
    "    \n",
    "    # Verify model outputs\n",
    "    verifier.verify_model_outputs(model, tokenizer, sample_sentence)\n",
    "    \n",
    "    # Get training suggestions\n",
    "    verifier.suggest_training_improvements(class_weights)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    verify_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f3b118c51904cf682452fb8bbe4608c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "196c1344837448b886a964e3effec00d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3006f612db344bc78d950116d223d954": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "377a5406e4e546d9a5468696dfaac0e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df4c4328930d41d1a2671dc866d6991f",
       "IPY_MODEL_e50cdc4d95aa4bcf9657c0c025eb01c8",
       "IPY_MODEL_9be769e2e057495bb06b1d0131baae0f"
      ],
      "layout": "IPY_MODEL_c692fe9eeefd4883be941ca4b8eece8d"
     }
    },
    "43c90cce99664de28746950a51d646ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53ab1b628ad94edd9a99b24ea74195f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5420b269ad2944f7a2fb00ca9721e0c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "600b850293be41faa99595e80af0dbd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6739a7301500410fab94144d5fd2c652": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea49321593cf47c08108b25f926287b1",
      "max": 17715,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f2527d8c162f42c7bd1bebdd0c0bdc07",
      "value": 17715
     }
    },
    "8e2a123ebf354b72849d32d9e619978a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99158fdbe3e2433ab1c2ac6e0b34ba8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9be769e2e057495bb06b1d0131baae0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53ab1b628ad94edd9a99b24ea74195f9",
      "placeholder": "​",
      "style": "IPY_MODEL_5420b269ad2944f7a2fb00ca9721e0c0",
      "value": " 44/886 [01:03&lt;18:21,  1.31s/it, loss=1.9231]"
     }
    },
    "bc61ec67db7348eb8250e53ac0e88092": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c541927c8ec54775b09c0d2fff45b130",
       "IPY_MODEL_6739a7301500410fab94144d5fd2c652",
       "IPY_MODEL_d1ced12f3ed54a0186ed46cc94e202f1"
      ],
      "layout": "IPY_MODEL_3006f612db344bc78d950116d223d954"
     }
    },
    "bd8e909fbf7b446788eeb59c6bbe911c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c541927c8ec54775b09c0d2fff45b130": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99158fdbe3e2433ab1c2ac6e0b34ba8d",
      "placeholder": "​",
      "style": "IPY_MODEL_0f3b118c51904cf682452fb8bbe4608c",
      "value": "Calculating sequence lengths: 100%"
     }
    },
    "c692fe9eeefd4883be941ca4b8eece8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1ced12f3ed54a0186ed46cc94e202f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e2a123ebf354b72849d32d9e619978a",
      "placeholder": "​",
      "style": "IPY_MODEL_196c1344837448b886a964e3effec00d",
      "value": " 17715/17715 [00:11&lt;00:00, 888.66it/s]"
     }
    },
    "dbe5db16cc204b5d8d800822a88f8199": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df4c4328930d41d1a2671dc866d6991f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43c90cce99664de28746950a51d646ef",
      "placeholder": "​",
      "style": "IPY_MODEL_dbe5db16cc204b5d8d800822a88f8199",
      "value": "Epoch 1/10 [Train]:   5%"
     }
    },
    "e50cdc4d95aa4bcf9657c0c025eb01c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_600b850293be41faa99595e80af0dbd2",
      "max": 886,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd8e909fbf7b446788eeb59c6bbe911c",
      "value": 44
     }
    },
    "ea49321593cf47c08108b25f926287b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2527d8c162f42c7bd1bebdd0c0bdc07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
