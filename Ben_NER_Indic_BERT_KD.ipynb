{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.19.3)\n",
      "Requirement already satisfied: sentencepiece in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: GPUtil in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: accelerate in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (2.20.0)\n",
      "Requirement already satisfied: setproctitle in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (72.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.24.5)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate) (0.4.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wandb sentencepiece GPUtil datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mP5oTzWaEAtx",
    "outputId": "a9072d1e-1696-4df5-9194-038cca31dffe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import wandb\n",
    "from huggingface_hub import HfApi\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from transformers import AutoConfig\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mvyexy_4EnQg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login with the token\n",
    "login(token=\"secret\")\n",
    "file_path= 'b-ner-train.csv'\n",
    "tag_to_id = {\n",
    "    'B-geo': 0, 'O': 1, 'B-gpe': 2, 'B-per': 3, 'I-per': 4, 'B-tim': 5,\n",
    "    'B-org': 6, 'I-org': 7, 'B-art': 8, 'I-art': 9, 'I-tim': 10,\n",
    "    'B-eve': 11, 'I-eve': 12, 'I-geo': 13, 'I-gpe': 14, 'B-nat': 15, 'I-nat': 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "780-44kxErTi"
   },
   "outputs": [],
   "source": [
    "class SentenceLengthAnalyzer:\n",
    "    def __init__(self, file_path, tokenizer):\n",
    "        self.file_path = file_path\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def analyze_lengths(self):\n",
    "        df = pd.read_csv(self.file_path)\n",
    "        sentences = df.groupby('Sentence #')['Word'].apply(list).values\n",
    "        \n",
    "        lengths = []\n",
    "        for sentence in tqdm(sentences, desc=\"Analyzing sentence lengths\"):\n",
    "            tokens = self.tokenizer.encode(\" \".join(sentence), add_special_tokens=True)\n",
    "            lengths.append(len(tokens))\n",
    "            \n",
    "        max_len = int(np.percentile(lengths, 99))\n",
    "        logger.info(f\"99th percentile length: {max_len}\")\n",
    "        return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "72JULpP7EzDD"
   },
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_len, tag_to_id):\n",
    "        self.data = pd.read_csv(file_path,nrows=300)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.tag_to_id = tag_to_id\n",
    "        \n",
    "        # Group by sentence and validate data\n",
    "        self.sentences = self.data.groupby('Sentence #').agg({\n",
    "            'Word': list,\n",
    "            'Tag': list\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Fix any length mismatches in sentences\n",
    "        self._fix_length_mismatches()\n",
    "        \n",
    "    def _fix_length_mismatches(self):\n",
    "        \"\"\"Ensure word and tag lists have matching lengths\"\"\"\n",
    "        for idx, row in self.sentences.iterrows():\n",
    "            words = row['Word']\n",
    "            tags = row['Tag']\n",
    "            if len(words) != len(tags):\n",
    "                min_len = min(len(words), len(tags))\n",
    "                self.sentences.at[idx, 'Word'] = words[:min_len]\n",
    "                self.sentences.at[idx, 'Tag'] = tags[:min_len]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences.iloc[idx]\n",
    "        words = sentence['Word']\n",
    "        tags = sentence['Tag']\n",
    "        \n",
    "        # Handle non-string inputs\n",
    "        text = \" \".join(str(word) for word in words)\n",
    "        \n",
    "        # Tokenize with truncation\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Initialize labels with ignore index\n",
    "        labels = torch.ones(self.max_len, dtype=torch.long) * -100\n",
    "        \n",
    "        # Get word IDs safely\n",
    "        word_ids = self.tokenizer(text, add_special_tokens=True).word_ids()\n",
    "        \n",
    "        # Handle label assignment safely\n",
    "        current_word_idx = None\n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            if i >= self.max_len:\n",
    "                break\n",
    "                \n",
    "            if word_idx is not None:\n",
    "                try:\n",
    "                    if word_idx < len(tags):  # Check if index is valid\n",
    "                        labels[i] = self.tag_to_id[tags[word_idx]]\n",
    "                except (IndexError, KeyError):\n",
    "                    # Default to 'O' tag for any errors\n",
    "                    labels[i] = self.tag_to_id['O']\n",
    "                current_word_idx = word_idx\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "V2MfqqFUE9Gg"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 2 (2369225868.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.model_name = model_name\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 2\n"
     ]
    }
   ],
   "source": [
    "class NERTrainer:\n",
    "    def __init__(self, model_name, tokenizer, max_len, tag_to_id, train_file, \n",
    "                 output_dir, wandb_key, device='cuda'):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.tag_to_id = tag_to_id\n",
    "        self.train_file = train_file\n",
    "        self.output_dir = output_dir\n",
    "        self.device = device\n",
    "        self.wandb_key = wandb_key\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # Get the config first\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(tag_to_id),\n",
    "            id2label={v: k for k, v in tag_to_id.items()},\n",
    "            label2id=tag_to_id,\n",
    "        )\n",
    "        \n",
    "        # Initialize model with config\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def _make_state_dict_contiguous(self, state_dict):\n",
    "        \"\"\"Make all tensors in state dict contiguous and on CPU\"\"\"\n",
    "        contiguous_state_dict = {}\n",
    "        for key, tensor in state_dict.items():\n",
    "            if isinstance(tensor, torch.Tensor):\n",
    "                # Move to CPU first if needed\n",
    "                if tensor.device.type != \"cpu\":\n",
    "                    tensor = tensor.cpu()\n",
    "                # Make contiguous and clone to ensure memory ownership\n",
    "                tensor = tensor.contiguous().clone()\n",
    "            contiguous_state_dict[key] = tensor\n",
    "        return contiguous_state_dict\n",
    "\n",
    "    def save_model(self, output_dir):\n",
    "        \"\"\"Custom model saving function\"\"\"\n",
    "        # Get model state dict\n",
    "        state_dict = self.model.state_dict()\n",
    "        \n",
    "        # Make all tensors contiguous\n",
    "        contiguous_state_dict = self._make_state_dict_contiguous(state_dict)\n",
    "        \n",
    "        # Save using PyTorch's native format\n",
    "        torch.save(contiguous_state_dict, os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "        \n",
    "        # Save config\n",
    "        self.model.config.save_pretrained(output_dir)\n",
    "        \n",
    "        # Save tokenizer\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    def train(self, batch_size=16, num_epochs=3, learning_rate=2e-5):\n",
    "        # Set up wandb\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        gpu_info = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        run_name = f\"Ben_NER_indic-bert_{current_time}_{gpu_info}\"\n",
    "        \n",
    "        wandb.login(key=self.wandb_key)\n",
    "        wandb.init(project=run_name)\n",
    "        \n",
    "        # Prepare dataset\n",
    "        train_dataset = NERDataset(self.train_file, self.tokenizer, self.max_len, self.tag_to_id)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=100,\n",
    "            save_strategy=\"epoch\",\n",
    "            report_to=\"wandb\",\n",
    "            fp16=True,\n",
    "            gradient_accumulation_steps=2,\n",
    "            save_safetensors=False,  # Disable safetensors\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Train the model\n",
    "            trainer.train()\n",
    "            \n",
    "            # Save model using custom save function\n",
    "            logger.info(\"Saving the model...\")\n",
    "            self.save_model(self.output_dir)\n",
    "            \n",
    "            logger.info(\"Model saved successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Training error: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5_2xn0AfFDYP"
   },
   "outputs": [],
   "source": [
    "class ModelPusher:\n",
    "    def __init__(self, model_path, repo_id, token):\n",
    "        self.model_path = model_path\n",
    "        self.repo_id = repo_id\n",
    "        self.token = token\n",
    "        self.api = HfApi()\n",
    "        \n",
    "    def push_to_hub(self):\n",
    "        try:\n",
    "            self.api.create_repo(repo_id=self.repo_id, exist_ok=True)\n",
    "            self.api.upload_folder(\n",
    "                folder_path=self.model_path,\n",
    "                repo_id=self.repo_id,\n",
    "                repo_type=\"model\",\n",
    "                token=self.token\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to push model: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "ECRklCzeFQeK",
    "outputId": "6bc27008-d992-4389-9b68-1d1375d987a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Analyzing sentence lengths: 100%|██████████| 17715/17715 [00:02<00:00, 8254.15it/s]\n",
      "INFO:__main__:99th percentile length: 110\n",
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wandb/run-20250114_151421-x9knpn74</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deb/Ben_NER_indic-bert_20250114_151420_NVIDIA%20RTX%20A5000/runs/x9knpn74' target=\"_blank\">radiant-snow-1</a></strong> to <a href='https://wandb.ai/deb/Ben_NER_indic-bert_20250114_151420_NVIDIA%20RTX%20A5000' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deb/Ben_NER_indic-bert_20250114_151420_NVIDIA%20RTX%20A5000' target=\"_blank\">https://wandb.ai/deb/Ben_NER_indic-bert_20250114_151420_NVIDIA%20RTX%20A5000</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deb/Ben_NER_indic-bert_20250114_151420_NVIDIA%20RTX%20A5000/runs/x9knpn74' target=\"_blank\">https://wandb.ai/deb/Ben_NER_indic-bert_20250114_151420_NVIDIA%20RTX%20A5000/runs/x9knpn74</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Training error: AlbertForTokenClassification does not support gradient checkpointing.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">radiant-snow-1</strong> at: <a href='https://wandb.ai/deb/Ben_NER_indic-bert_20250114_151420_NVIDIA%20RTX%20A5000/runs/x9knpn74' target=\"_blank\">https://wandb.ai/deb/Ben_NER_indic-bert_20250114_151420_NVIDIA%20RTX%20A5000/runs/x9knpn74</a><br> View project at: <a href='https://wandb.ai/deb/Ben_NER_indic-bert_20250114_151420_NVIDIA%20RTX%20A5000' target=\"_blank\">https://wandb.ai/deb/Ben_NER_indic-bert_20250114_151420_NVIDIA%20RTX%20A5000</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250114_151421-x9knpn74/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "AlbertForTokenClassification does not support gradient checkpointing.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 21\u001b[0m\n\u001b[1;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m NERTrainer(\n\u001b[1;32m     11\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai4bharat/indic-bert\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     wandb_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbb1fc2f9f6a2a5c9a636f32edb2c4b1de8598d78\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Push to Hub\u001b[39;00m\n\u001b[1;32m     24\u001b[0m pusher \u001b[38;5;241m=\u001b[39m ModelPusher(\n\u001b[1;32m     25\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model_output\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDebk/Ben_NER_indic-bert\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_ScigEtJrBeLYPBKTmdBeundzdgBJRohkxM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n",
      "Cell \u001b[0;32mIn[6], line 64\u001b[0m, in \u001b[0;36mNERTrainer.train\u001b[0;34m(self, batch_size, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     56\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     57\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     58\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     59\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Make tensors contiguous before saving\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking tensors contiguous before saving...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1946\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1949\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py:2083\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2080\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2081\u001b[0m         gradient_checkpointing_kwargs \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_checkpointing_kwargs\n\u001b[0;32m-> 2083\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_checkpointing_enable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient_checkpointing_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_checkpointing_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2085\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped)\n\u001b[1;32m   2087\u001b[0m \u001b[38;5;66;03m# as the model is wrapped, don't use `accelerator.prepare`\u001b[39;00m\n\u001b[1;32m   2088\u001b[0m \u001b[38;5;66;03m# this is for unhandled cases such as\u001b[39;00m\n\u001b[1;32m   2089\u001b[0m \u001b[38;5;66;03m# FSDP-XLA, SageMaker MP/DP, DataParallel, IPEX\u001b[39;00m\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:2337\u001b[0m, in \u001b[0;36mPreTrainedModel.gradient_checkpointing_enable\u001b[0;34m(self, gradient_checkpointing_kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;124;03mActivates gradient checkpointing for the current model.\u001b[39;00m\n\u001b[1;32m   2325\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[38;5;124;03m        Additional keyword arguments passed along to the `torch.utils.checkpoint.checkpoint` function.\u001b[39;00m\n\u001b[1;32m   2335\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_gradient_checkpointing:\n\u001b[0;32m-> 2337\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not support gradient checkpointing.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gradient_checkpointing_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2340\u001b[0m     gradient_checkpointing_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n",
      "\u001b[0;31mValueError\u001b[0m: AlbertForTokenClassification does not support gradient checkpointing."
     ]
    }
   ],
   "source": [
    "#def main():\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "\n",
    "# Analyze sentence lengths\n",
    "analyzer = SentenceLengthAnalyzer(file_path, tokenizer)\n",
    "max_len = analyzer.analyze_lengths()\n",
    "\n",
    "trainer = NERTrainer(\n",
    "    model_name=\"ai4bharat/indic-bert\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len,\n",
    "    tag_to_id=tag_to_id,\n",
    "    train_file=file_path,\n",
    "    output_dir=\"./model_output\",\n",
    "    wandb_key=\"secret\"\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train(num_epochs=1)\n",
    "\n",
    "# Push to Hub\n",
    "pusher = ModelPusher(\n",
    "    model_path=\"./model_output\",\n",
    "    repo_id=\"Debk/Ben_NER_indic-bert\",\n",
    "    token=\"secret\"\n",
    ")\n",
    "pusher.push_to_hub()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
