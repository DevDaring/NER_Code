{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUU2Gv0_GfDL",
        "outputId": "46797a31-e01b-4c7c-e958-2b10260bb26f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch-crf\n",
            "  Downloading pytorch_crf-0.7.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.6.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: seqeval, GPUtil\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=5dbf828f4f8ac669aedc4099f83fdb3995aed28457ad7b46559478bd0975af10\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=0e2fc230780e3e805ef0d5e730a0d43ca14d7384d40b781ac4ef11de5d1b2e04\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built seqeval GPUtil\n",
            "Installing collected packages: GPUtil, xxhash, pytorch-crf, fsspec, dill, multiprocess, seqeval, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GPUtil-1.4.0 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 pytorch-crf-0.7.2 seqeval-1.2.2 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-crf datasets seqeval tqdm wandb GPUtil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmjPztogF4Hv",
        "outputId": "a18916e8-1f5c-4bae-d2df-000ebe3fd19f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import XLMRobertaTokenizerFast,XLMRobertaModel\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import logging\n",
        "import torch.nn as nn\n",
        "from datetime import datetime\n",
        "from huggingface_hub import HfFolder, HfApi\n",
        "from huggingface_hub import hf_hub_download\n",
        "from datasets import Dataset as HFDataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "tag_to_id = {\n",
        "    'B-geo': 0, 'O': 1, 'B-gpe': 2, 'B-per': 3, 'I-per': 4, 'B-tim': 5,\n",
        "    'B-org': 6, 'I-org': 7, 'B-art': 8, 'I-art': 9, 'I-tim': 10,\n",
        "    'B-eve': 11, 'I-eve': 12, 'I-geo': 13, 'I-gpe': 14, 'B-nat': 15, 'I-nat': 16\n",
        "}\n",
        "id_to_tag = {v: k for k, v in tag_to_id.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93ySMk_eGkKq"
      },
      "outputs": [],
      "source": [
        "class NERDataset(Dataset):\n",
        "    def __init__(self, texts, tags, tokenizer, tag_to_id, max_len):\n",
        "        self.texts = texts\n",
        "        self.tags = tags\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tag_to_id = tag_to_id\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            words = self.texts[idx]\n",
        "            tags = self.tags[idx]\n",
        "\n",
        "            encoding = self.tokenizer(\n",
        "                words,\n",
        "                is_split_into_words=True,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                max_length=self.max_len,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            label_ids = []\n",
        "            word_ids = encoding.word_ids()\n",
        "\n",
        "            for word_idx in word_ids:\n",
        "                if word_idx is None:\n",
        "                    label_ids.append(-100)\n",
        "                else:\n",
        "                    label_ids.append(self.tag_to_id[tags[word_idx]])\n",
        "\n",
        "            encoding = {key: val.squeeze() for key, val in encoding.items()}\n",
        "            encoding['labels'] = torch.tensor(label_ids)\n",
        "\n",
        "            return encoding\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing item {idx}: {e}\")\n",
        "            raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZtZwymuH2as"
      },
      "outputs": [],
      "source": [
        "class XLMRobertaBiLSTM(nn.Module):\n",
        "    def __init__(self, num_labels, dropout=0.1, lstm_hidden_size=256):\n",
        "        super().__init__()\n",
        "        self.roberta = XLMRobertaModel.from_pretrained('xlm-roberta-large')\n",
        "        hidden_size = self.roberta.config.hidden_size\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=lstm_hidden_size,\n",
        "            num_layers=2,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if 2 > 1 else 0\n",
        "        )\n",
        "        self.classifier = nn.Linear(lstm_hidden_size * 2, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs[0]\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "\n",
        "        lstm_output, _ = self.lstm(sequence_output)\n",
        "        lstm_output = self.dropout(lstm_output)\n",
        "\n",
        "        logits = self.classifier(lstm_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            active_loss = labels.view(-1) != -100\n",
        "            active_logits = logits.view(-1, logits.shape[-1])\n",
        "            active_labels = labels.view(-1)\n",
        "            loss = loss_fct(active_logits[active_loss], active_labels[active_loss])\n",
        "\n",
        "        return {'loss': loss, 'logits': logits} if loss is not None else {'logits': logits}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShIXW1SjkBXT"
      },
      "outputs": [],
      "source": [
        "class ModelTester:\n",
        "    def __init__(self, model_id, token, tag_to_id, device, tokenizer):\n",
        "        self.model_id = model_id\n",
        "        self.token = token\n",
        "        self.tag_to_id = tag_to_id\n",
        "        self.id_to_tag = {v: k for k, v in tag_to_id.items()}\n",
        "        self.device = device\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        try:\n",
        "            model = XLMRobertaBiLSTM(num_labels=len(self.tag_to_id))\n",
        "\n",
        "            from huggingface_hub import hf_hub_download\n",
        "            model_path = hf_hub_download(\n",
        "                repo_id=self.model_id,\n",
        "                filename=\"pytorch_model.bin\",\n",
        "                token=self.token\n",
        "            )\n",
        "\n",
        "            state_dict = torch.load(model_path, map_location=self.device)\n",
        "            model.load_state_dict(state_dict)\n",
        "\n",
        "            model = model.to(self.device)\n",
        "            model.eval()\n",
        "            logger.info(f\"Successfully loaded model from {self.model_id}\")\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model from HuggingFace: {str(e)}\")\n",
        "            raise e\n",
        "\n",
        "    def predict_sentence(self, words):\n",
        "        \"\"\"Predict tags for a single sentence\"\"\"\n",
        "        try:\n",
        "            # Tokenize the sentence\n",
        "            encoding = self.tokenizer(\n",
        "                words,\n",
        "                is_split_into_words=True,\n",
        "                return_tensors='pt',\n",
        "                padding=True,\n",
        "                truncation=True\n",
        "            )\n",
        "\n",
        "            # Move to device\n",
        "            input_ids = encoding['input_ids'].to(self.device)\n",
        "            attention_mask = encoding['attention_mask'].to(self.device)\n",
        "\n",
        "            # Get predictions\n",
        "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs['logits']\n",
        "\n",
        "            # Get probabilities using softmax\n",
        "            probs = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
        "            predictions = torch.argmax(logits, dim=2)[0]\n",
        "\n",
        "            # Get word-level predictions and confidences\n",
        "            word_predictions = []\n",
        "            word_confidences = []\n",
        "\n",
        "            # Map predictions back to words\n",
        "            word_ids = encoding.word_ids(0)  # Get word_ids for first sequence\n",
        "            previous_word_idx = None\n",
        "            max_prob = 0\n",
        "            current_pred = None\n",
        "\n",
        "            for idx, word_idx in enumerate(word_ids):\n",
        "                if word_idx is None:\n",
        "                    continue\n",
        "\n",
        "                if word_idx != previous_word_idx:\n",
        "                    if previous_word_idx is not None:\n",
        "                        word_predictions.append(self.id_to_tag[current_pred])\n",
        "                        word_confidences.append(max_prob)\n",
        "                    max_prob = float(torch.max(probs[idx]))\n",
        "                    current_pred = int(predictions[idx])\n",
        "                    previous_word_idx = word_idx\n",
        "                else:\n",
        "                    # If it's the same word, update if probability is higher\n",
        "                    prob = float(torch.max(probs[idx]))\n",
        "                    if prob > max_prob:\n",
        "                        max_prob = prob\n",
        "                        current_pred = int(predictions[idx])\n",
        "\n",
        "            # Add the last word\n",
        "            if current_pred is not None:\n",
        "                word_predictions.append(self.id_to_tag[current_pred])\n",
        "                word_confidences.append(max_prob)\n",
        "\n",
        "            # Verify we have a prediction for each word\n",
        "            if len(word_predictions) != len(words):\n",
        "                logger.warning(f\"Mismatch in sentence predictions: got {len(word_predictions)} predictions for {len(words)} words\")\n",
        "                return ['O'] * len(words), [0.0] * len(words)\n",
        "\n",
        "            return word_predictions, word_confidences\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in predict_sentence: {str(e)}\")\n",
        "            return ['O'] * len(words), [0.0] * len(words)\n",
        "\n",
        "    def predict_tags(self, test_loader, test_df):\n",
        "        \"\"\"\n",
        "        Predict tags for test data\n",
        "        Returns:\n",
        "            word_predictions: list of predicted tags\n",
        "            word_true_labels: list of true tags\n",
        "            word_confidences: list of confidence scores\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting prediction...\")\n",
        "        all_predictions = []\n",
        "        all_confidences = []\n",
        "\n",
        "        # Process each sentence\n",
        "        sentences = test_df.groupby('Sentence #')['Word'].apply(list).values\n",
        "\n",
        "        for sentence_words in tqdm(sentences, desc=\"Processing sentences\"):\n",
        "            predictions, confidences = self.predict_sentence(sentence_words)\n",
        "            all_predictions.extend(predictions)\n",
        "            all_confidences.extend(confidences)\n",
        "\n",
        "        # Get true labels\n",
        "        true_labels = test_df['Tag'].tolist()\n",
        "\n",
        "        # Verify predictions match the number of words\n",
        "        assert len(all_predictions) == len(test_df), \\\n",
        "            f\"Mismatch between predictions ({len(all_predictions)}) and words ({len(test_df)})\"\n",
        "\n",
        "        # Log prediction distribution\n",
        "        tag_counts = {}\n",
        "        for tag in all_predictions:\n",
        "            tag_counts[tag] = tag_counts.get(tag, 0) + 1\n",
        "\n",
        "        logger.info(\"\\nPrediction distribution:\")\n",
        "        for tag, count in tag_counts.items():\n",
        "            percentage = (count / len(all_predictions)) * 100\n",
        "            logger.info(f\"{tag}: {count} ({percentage:.2f}%)\")\n",
        "\n",
        "        return all_predictions, true_labels, all_confidences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nmLAFFo9awl"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503,
          "referenced_widgets": [
            "75cfd04354f84d6f975ac7c4af2a4c37",
            "2fd7792fdba6414c8ed6412bb374c96b",
            "befdc3f8e4444ab793dac4d8bd3d8b7e",
            "4ffd08243fa047e99c0e8256a5637292",
            "05e6ec395b0a496a8318b3a162e7f07c",
            "1722d975248d4b9b8374c8af324b392c",
            "a0e562bf702649b2a9aa8345d22ad9c1",
            "d91ac243a74248b489186c81b9e58c28",
            "9012107c65b0421ab5fa580a475ea853",
            "b360e4b0a00944379ff63abc09c38209",
            "f0aa975ff5634224a8e2e0a032f495f2"
          ]
        },
        "id": "D-ibb_mPIT19",
        "outputId": "1b3c14c1-0bcd-4db3-d659-ba2dc0953db4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-ad4847e8fc75>:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=self.device)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75cfd04354f84d6f975ac7c4af2a4c37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing sentences:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model: XLM-RoBERTa-Large with BiLSTM\n",
            "Model ID: Debk/Ben_NER_xlm-roberta-large_BiLSTM\n",
            "Test Date: 2025-01-14 08:40:43\n",
            "Number of Test Sentences: 33\n",
            "\n",
            "Metrics (Macro):\n",
            "---------------\n",
            "Accuracy: 0.0067\n",
            "Precision: 0.0005\n",
            "Recall: 0.0769\n",
            "F1 Score: 0.0010\n",
            "\n",
            "Confidence Analysis:\n",
            "-------------------\n",
            "Average Confidence: 0.0674\n",
            "Average Confidence (Correct Predictions): 0.0674\n",
            "Average Confidence (Incorrect Predictions): 0.0674\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##def main():\n",
        "# Initialize device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-large')\n",
        "# Load test data\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/NER_Dataset/b-ner-test.csv')\n",
        "test_sentences = test_df.groupby('Sentence #')['Word'].apply(list).values\n",
        "test_tags = test_df.groupby('Sentence #')['Tag'].apply(list).values\n",
        "num_sentences = len(test_sentences)\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = NERDataset(test_sentences, test_tags, tokenizer, tag_to_id, max_len=400)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize tester with tokenizer\n",
        "tester = ModelTester(\n",
        "    model_id=\"Debk/Ben_NER_xlm-roberta-large_BiLSTM\",\n",
        "    token=\"secret\",\n",
        "    tag_to_id=tag_to_id,\n",
        "    device=device,\n",
        "    tokenizer=tokenizer  # Pass the tokenizer\n",
        ")\n",
        "predicted_tags, true_tags, confidences = tester.predict_tags(test_loader, test_df)\n",
        "\n",
        "# Add predictions and confidences to DataFrame\n",
        "test_df['BiLSTM_Roberta_Pred'] = predicted_tags\n",
        "test_df['confidence'] = confidences\n",
        "\n",
        "# Save results with confidence\n",
        "test_df.to_csv('/content/drive/MyDrive/NER_Dataset/BiLSTM_Roberta_result_test.csv', index=False)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(true_tags, predicted_tags)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    true_tags, predicted_tags, average='macro'\n",
        ")\n",
        "\n",
        "# Create results text file with confidence information\n",
        "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Calculate confidence statistics\n",
        "avg_confidence = np.mean(confidences)\n",
        "avg_confidence_correct = np.mean([conf for conf, pred, true in\n",
        "                                zip(confidences, predicted_tags, true_tags)\n",
        "                                if pred == true])\n",
        "avg_confidence_incorrect = np.mean([conf for conf, pred, true in\n",
        "                                  zip(confidences, predicted_tags, true_tags)\n",
        "                                  if pred != true])\n",
        "\n",
        "results_text = f\"\"\"\n",
        "Model: XLM-RoBERTa-Large with BiLSTM\n",
        "Model ID: Debk/Ben_NER_xlm-roberta-large_BiLSTM\n",
        "Test Date: {current_time}\n",
        "Number of Test Sentences: {num_sentences}\n",
        "\n",
        "Metrics (Macro):\n",
        "---------------\n",
        "Accuracy: {accuracy:.4f}\n",
        "Precision: {precision:.4f}\n",
        "Recall: {recall:.4f}\n",
        "F1 Score: {f1:.4f}\n",
        "\n",
        "Confidence Analysis:\n",
        "-------------------\n",
        "Average Confidence: {avg_confidence:.4f}\n",
        "Average Confidence (Correct Predictions): {avg_confidence_correct:.4f}\n",
        "Average Confidence (Incorrect Predictions): {avg_confidence_incorrect:.4f}\n",
        "\"\"\"\n",
        "\n",
        "# Save metrics\n",
        "with open('/content/drive/MyDrive/NER_Dataset/BiLSTM_Roberta_result_test.txt', 'w') as f:\n",
        "    f.write(results_text)\n",
        "\n",
        "logger.info(\"Testing completed. Results saved to files.\")\n",
        "print(results_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUjOuxWEadWm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05e6ec395b0a496a8318b3a162e7f07c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1722d975248d4b9b8374c8af324b392c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fd7792fdba6414c8ed6412bb374c96b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1722d975248d4b9b8374c8af324b392c",
            "placeholder": "​",
            "style": "IPY_MODEL_a0e562bf702649b2a9aa8345d22ad9c1",
            "value": "Processing sentences: 100%"
          }
        },
        "4ffd08243fa047e99c0e8256a5637292": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b360e4b0a00944379ff63abc09c38209",
            "placeholder": "​",
            "style": "IPY_MODEL_f0aa975ff5634224a8e2e0a032f495f2",
            "value": " 33/33 [00:01&lt;00:00, 34.56it/s]"
          }
        },
        "75cfd04354f84d6f975ac7c4af2a4c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2fd7792fdba6414c8ed6412bb374c96b",
              "IPY_MODEL_befdc3f8e4444ab793dac4d8bd3d8b7e",
              "IPY_MODEL_4ffd08243fa047e99c0e8256a5637292"
            ],
            "layout": "IPY_MODEL_05e6ec395b0a496a8318b3a162e7f07c"
          }
        },
        "9012107c65b0421ab5fa580a475ea853": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0e562bf702649b2a9aa8345d22ad9c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b360e4b0a00944379ff63abc09c38209": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "befdc3f8e4444ab793dac4d8bd3d8b7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d91ac243a74248b489186c81b9e58c28",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9012107c65b0421ab5fa580a475ea853",
            "value": 33
          }
        },
        "d91ac243a74248b489186c81b9e58c28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0aa975ff5634224a8e2e0a032f495f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
